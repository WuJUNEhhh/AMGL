{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from emvert_functional_code import emvert_ml\n",
    "from scipy.sparse import csgraph\n",
    "import csv\n",
    "import shutil\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "import gc\n",
    "import matplotlib.cm\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import scipy.sparse as spsprs\n",
    "import torch\n",
    "import torch.autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from nilearn import connectome\n",
    "from collections import Counter\n",
    "from nilearn import datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = 'cpac'\n",
    "\n",
    "# Input data variables\n",
    "root_folder = '/data/zfzhu/zs/MMGL/miccai_ABIDE/data'\n",
    "data_folder = os.path.join(root_folder, 'ABIDE_pcp/cpac/filt_noglobal')\n",
    "phenotype = os.path.join(root_folder, 'ABIDE_pcp/Phenotypic_V1_0b_preprocessed1.csv')\n",
    "#phenotype = os.path.join(root_folder, 'TADPOLE/processed_standard_data.csv')\n",
    "def fetch_filenames(subject_IDs, file_type):\n",
    "\n",
    "    \"\"\"\n",
    "        subject_list : list of short subject IDs in string format\n",
    "        file_type    : must be one of the available file types\n",
    "    returns:\n",
    "        filenames    : list of filetypes (same length as subject_list)\n",
    "    \"\"\"\n",
    "\n",
    "    import glob\n",
    "\n",
    "    # Specify file mappings for the possible file types\n",
    "    filemapping = {'func_preproc': '_func_preproc.nii.gz',\n",
    "                   'rois_ho': '_rois_ho.1D'}\n",
    "\n",
    "    # The list to be filled\n",
    "    filenames = []\n",
    "\n",
    "    # Fill list with requested file paths\n",
    "    for i in range(len(subject_IDs)):\n",
    "        \n",
    "        os.chdir(os.path.join(data_folder, subject_IDs[i]))  # os.path.join(data_folder, subject_IDs[i]))\n",
    "        try:\n",
    "            filenames.append(glob.glob('*' + subject_IDs[i] + filemapping[file_type])[0])\n",
    "        except IndexError:\n",
    "            # Return N/A if subject ID is not found\n",
    "            filenames.append('N/A')\n",
    "\n",
    "    return filenames\n",
    "\n",
    "\n",
    "# Get timeseries arrays for list of subjects\n",
    "def get_timeseries(subject_list, atlas_name):\n",
    "    \"\"\"\n",
    "        subject_list : list of short subject IDs in string format\n",
    "        atlas_name   : the atlas based on which the timeseries are generated e.g. aal, cc200\n",
    "    returns:\n",
    "        time_series  : list of timeseries arrays, each of shape (timepoints x regions)\n",
    "    \"\"\"\n",
    "\n",
    "    timeseries = []\n",
    "    for i in range(len(subject_list)):\n",
    "        subject_folder = os.path.join(data_folder, subject_list[i])\n",
    "        ro_file = [f for f in os.listdir(subject_folder) if f.endswith('_rois_' + atlas_name + '.1D')]\n",
    "        fl = os.path.join(subject_folder, ro_file[0])\n",
    "        print(\"Reading timeseries file %s\" %fl)\n",
    "        timeseries.append(np.loadtxt(fl, skiprows=0))\n",
    "\n",
    "    return timeseries\n",
    "\n",
    "\n",
    "# Compute connectivity matrices\n",
    "def subject_connectivity(timeseries, subject, atlas_name, kind, save=True, save_path=data_folder):\n",
    "    \"\"\"\n",
    "        timeseries   : timeseries table for subject (timepoints x regions)\n",
    "        subject      : the subject ID\n",
    "        atlas_name   : name of the parcellation atlas used\n",
    "        kind         : the kind of connectivity to be used, e.g. lasso, partial correlation, correlation\n",
    "        save         : save the connectivity matrix to a file\n",
    "        save_path    : specify path to save the matrix if different from subject folder\n",
    "    returns:\n",
    "        connectivity : connectivity matrix (regions x regions)\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Estimating %s matrix for subject %s\" % (kind, subject))\n",
    "\n",
    "    if kind in ['tangent', 'partial correlation', 'correlation']:\n",
    "        conn_measure = connectome.ConnectivityMeasure(kind=kind)\n",
    "        connectivity = conn_measure.fit_transform([timeseries])[0]\n",
    "\n",
    "    if save:\n",
    "        subject_file = os.path.join(save_path, subject,\n",
    "                                    subject + '_' + atlas_name + '_' + kind.replace(' ', '_') + '.mat')\n",
    "        sio.savemat(subject_file, {'connectivity': connectivity})\n",
    "\n",
    "    return connectivity\n",
    "\n",
    "\n",
    "# Get the list of subject IDs\n",
    "def get_ids(num_subjects=None):\n",
    "    \"\"\"\n",
    "    return:\n",
    "        subject_IDs    : list of all subject IDs\n",
    "    \"\"\"\n",
    "\n",
    "    subject_IDs = np.genfromtxt(os.path.join(data_folder, 'subject_IDs.txt'), dtype=str)\n",
    "\n",
    "    if num_subjects is not None:\n",
    "        subject_IDs = subject_IDs[:num_subjects]\n",
    "\n",
    "    return subject_IDs\n",
    "\n",
    "\n",
    "# Get phenotype values for a list of subjects\n",
    "def get_subject_score(subject_list, score):\n",
    "    scores_dict = {}\n",
    "\n",
    "    with open(phenotype) as csv_file:\n",
    "        reader = csv.DictReader(csv_file)\n",
    "        for row in reader:\n",
    "            if row['SUB_ID'] in subject_list:\n",
    "                scores_dict[row['SUB_ID']] = row[score]\n",
    "\n",
    "    return scores_dict\n",
    "\n",
    "\n",
    "# Dimensionality reduction step for the feature vector using a ridge classifier\n",
    "def feature_selection(matrix, labels, train_ind, fnum):\n",
    "    \"\"\"\n",
    "        matrix       : feature matrix (num_subjects x num_features)\n",
    "        labels       : ground truth labels (num_subjects x 1)\n",
    "        train_ind    : indices of the training samples\n",
    "        fnum         : size of the feature vector after feature selection\n",
    "    return:\n",
    "        x_data      : feature matrix of lower dimension (num_subjects x fnum)\n",
    "    \"\"\"\n",
    "\n",
    "    estimator = RidgeClassifier()\n",
    "    selector = RFE(estimator, fnum, step=100, verbose=1)\n",
    "    print(np.shape(train_ind))\n",
    "    print(train_ind)\n",
    "    print(\"at ferature selection\")\n",
    "\n",
    "    featureX = matrix[train_ind, :]\n",
    "    featureY = labels[train_ind]\n",
    "    selector = selector.fit(featureX, featureY.ravel())\n",
    "    x_data = selector.transform(matrix)\n",
    "\n",
    "    print(\"Number of labeled samples %d\" % len(train_ind))\n",
    "    print(\"Number of features selected %d\" % x_data.shape[1])\n",
    "\n",
    "    return x_data\n",
    "\n",
    "\n",
    "# Make sure each site is represented in the training set when selecting a subset of the training set\n",
    "def site_percentage(train_ind, perc, subject_list):\n",
    "    \"\"\"\n",
    "        train_ind    : indices of the training samples\n",
    "        perc         : percentage of training set used\n",
    "        subject_list : list of subject IDs\n",
    "    return:\n",
    "        labeled_indices      : indices of the subset of training samples\n",
    "    \"\"\"\n",
    "\n",
    "    train_list = subject_list[train_ind]\n",
    "    sites = get_subject_score(train_list, score='SITE_ID')\n",
    "    unique = np.unique(list(sites.values())).tolist()\n",
    "    site = np.array([unique.index(sites[train_list[x]]) for x in range(len(train_list))])\n",
    "\n",
    "    labeled_indices = []\n",
    "\n",
    "    for i in np.unique(site):\n",
    "        id_in_site = np.argwhere(site == i).flatten()\n",
    "\n",
    "        num_nodes = len(id_in_site)\n",
    "        labeled_num = int(round(perc * num_nodes))\n",
    "        labeled_indices.extend(train_ind[id_in_site[:labeled_num]])\n",
    "\n",
    "    return labeled_indices\n",
    "\n",
    "def feature_selection(matrix, labels, train_ind, fnum):\n",
    "    \"\"\"\n",
    "        matrix       : feature matrix (num_subjects x num_features)\n",
    "        labels       : ground truth labels (num_subjects x 1)\n",
    "        train_ind    : indices of the training samples\n",
    "        fnum         : size of the feature vector after feature selection\n",
    "    return:\n",
    "        x_data      : feature matrix of lower dimension (num_subjects x fnum)\n",
    "    \"\"\"\n",
    "\n",
    "    estimator = RidgeClassifier()\n",
    "    selector = RFE(estimator, fnum, step=100, verbose=1)\n",
    "\n",
    "    featureX = matrix[train_ind, :]\n",
    "    featureY = labels[train_ind]\n",
    "    selector = selector.fit(featureX, featureY.ravel())\n",
    "    x_data = selector.transform(matrix)\n",
    "\n",
    "    print(\"Number of labeled samples %d\" % len(train_ind))\n",
    "    print(\"Number of features selected %d\" % x_data.shape[1])\n",
    "\n",
    "    return x_data\n",
    "\n",
    "# Load precomputed fMRI connectivity networks\n",
    "def get_networks(subject_list, kind, atlas_name=\"aal\", variable='connectivity'):\n",
    "    \"\"\"\n",
    "        subject_list : list of subject IDs\n",
    "        kind         : the kind of connectivity to be used, e.g. lasso, partial correlation, correlation\n",
    "        atlas_name   : name of the parcellation atlas used\n",
    "        variable     : variable name in the .mat file that has been used to save the precomputed networks\n",
    "    return:\n",
    "        matrix      : feature matrix of connectivity networks (num_subjects x network_size)\n",
    "    \"\"\"\n",
    "\n",
    "    all_networks = []\n",
    "    for subject in subject_list:\n",
    "        fl = os.path.join(data_folder, subject,\n",
    "                          subject + \"_\" + atlas_name + \"_\" + kind + \".mat\")\n",
    "        matrix = sio.loadmat(fl)[variable]\n",
    "        all_networks.append(matrix)\n",
    "    # all_networks=np.array(all_networks)\n",
    "\n",
    "    idx = np.triu_indices_from(all_networks[0], 1)\n",
    "    norm_networks = [np.arctanh(mat) for mat in all_networks]\n",
    "    vec_networks = [mat[idx] for mat in norm_networks]\n",
    "    matrix = np.vstack(vec_networks)\n",
    "\n",
    "    return matrix\n",
    "\n",
    "def create_affinity_graph_from_scores(scores, subject_list):\n",
    "    \"\"\"\n",
    "        scores       : list of phenotypic information to be used to construct the affinity graph\n",
    "        subject_list : list of subject IDs\n",
    "\n",
    "    return:\n",
    "        graph        : adjacency matrix of the population graph (num_subjects x num_subjects)\n",
    "    \"\"\"\n",
    "\n",
    "    num_nodes = len(subject_list)\n",
    "    graph = np.zeros((num_nodes, num_nodes))\n",
    "\n",
    "    for l in scores:\n",
    "        label_dict = get_subject_score(subject_list, l)\n",
    "\n",
    "        # quantitative phenotypic scores\n",
    "        if l in ['AGE_AT_SCAN', 'FIQ']:\n",
    "            for k in range(num_nodes):\n",
    "                for j in range(k + 1, num_nodes):\n",
    "                    try:\n",
    "                        val = abs(float(label_dict[subject_list[k]]) - float(label_dict[subject_list[j]]))\n",
    "                        if val < 2:\n",
    "                            graph[k, j] += 1\n",
    "                            graph[j, k] += 1\n",
    "                    except ValueError:  # missing label\n",
    "                        pass\n",
    "\n",
    "        else:\n",
    "            for k in range(num_nodes):\n",
    "                for j in range(k + 1, num_nodes):\n",
    "                    if label_dict[subject_list[k]] == label_dict[subject_list[j]]:\n",
    "                        graph[k, j] += 1\n",
    "                        graph[j, k] += 1\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './subject_IDs.txt'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [10]\u001B[0m, in \u001B[0;36m<cell line: 7>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      3\u001B[0m filemapping \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfunc_preproc\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfunc_preproc.nii.gz\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m      4\u001B[0m                \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrois_ho\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrois_ho.1D\u001B[39m\u001B[38;5;124m'\u001B[39m}\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(data_folder): os\u001B[38;5;241m.\u001B[39mmakedirs(data_folder)\n\u001B[1;32m----> 7\u001B[0m \u001B[43mshutil\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcopyfile\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m./subject_IDs.txt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_folder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msubject_IDs.txt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\pytorch\\lib\\shutil.py:264\u001B[0m, in \u001B[0;36mcopyfile\u001B[1;34m(src, dst, follow_symlinks)\u001B[0m\n\u001B[0;32m    262\u001B[0m     os\u001B[38;5;241m.\u001B[39msymlink(os\u001B[38;5;241m.\u001B[39mreadlink(src), dst)\n\u001B[0;32m    263\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 264\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msrc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m fsrc:\n\u001B[0;32m    265\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    266\u001B[0m             \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(dst, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwb\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m fdst:\n\u001B[0;32m    267\u001B[0m                 \u001B[38;5;66;03m# macOS\u001B[39;00m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: './subject_IDs.txt'"
     ]
    }
   ],
   "source": [
    "files = ['rois_ho']\n",
    "num_subjects = 871\n",
    "filemapping = {'func_preproc': 'func_preproc.nii.gz',\n",
    "               'rois_ho': 'rois_ho.1D'}\n",
    "\n",
    "if not os.path.exists(data_folder): os.makedirs(data_folder)\n",
    "shutil.copyfile('./subject_IDs.txt', os.path.join(data_folder, 'subject_IDs.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Download database files\n",
    "abide = datasets.fetch_abide_pcp(data_dir=root_folder, n_subjects=num_subjects, pipeline=pipeline,\n",
    "                                 band_pass_filtering=True, global_signal_regression=False, derivatives=files)\n",
    "\n",
    "\n",
    "#subject_IDs = Reader.get_ids(num_subjects)\n",
    "#subject_IDs = subject_IDs.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "subject_IDs = get_ids()\n",
    "subject_IDs = subject_IDs.tolist()\n",
    "labels = get_subject_score(subject_IDs, score='DX_GROUP')\n",
    "\n",
    "for s, fname in zip(subject_IDs, fetch_filenames(subject_IDs, files[0])):\n",
    "    subject_folder = os.path.join(data_folder, s)\n",
    "    if not os.path.exists(subject_folder):\n",
    "        os.mkdir(subject_folder)\n",
    "\n",
    "    # Get the base filename for each subject\n",
    "    base = fname.split(files[0])[0]\n",
    "\n",
    "    # Move each subject file to the subject folder\n",
    "    for fl in files:\n",
    "        if not os.path.exists(os.path.join(subject_folder, base + filemapping[fl])):\n",
    "\n",
    "            shutil.move(base + filemapping[fl], subject_folder)\n",
    "\n",
    "time_series = get_timeseries(subject_IDs, 'ho')\n",
    "\n",
    "# Compute and save connectivity matrices\n",
    "for i in range(len(subject_IDs)):\n",
    "        subject_connectivity(time_series[i], subject_IDs[i], 'ho', 'correlation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEX   = get_subject_score(subject_IDs, score='SEX')\n",
    "AGE   = get_subject_score(subject_IDs, score='AGE_AT_SCAN')\n",
    "FIQ   = get_subject_score(subject_IDs, score='FIQ')\n",
    "LABEL = get_subject_score(subject_IDs, score='DX_GROUP')\n",
    "EYE_S = get_subject_score(subject_IDs, score='EYE_STATUS_AT_SCAN')\n",
    "BMI   = get_subject_score(subject_IDs, score='BMI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADS = []\n",
    "Normal = []\n",
    "for i in LABEL:\n",
    "    if LABEL[i] == '1':\n",
    "        ADS.append(i)\n",
    "    elif LABEL[i] == '2':\n",
    "        Normal.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Get acquisition site\n",
    "sites = get_subject_score(subject_IDs, score='SITE_ID')\n",
    "unique = np.unique(list(sites.values())).tolist()\n",
    "\n",
    "num_classes = 2\n",
    "num_nodes = len(subject_IDs)\n",
    "\n",
    "# Initialise variables for class labels and acquisition sites\n",
    "y_data = np.zeros([num_nodes, num_classes])\n",
    "y = np.zeros([num_nodes, 1])\n",
    "site = np.zeros([num_nodes, 1], dtype=np.int)\n",
    "\n",
    "# Get class labels and acquisition site for all subjects\n",
    "for i in range(num_nodes):\n",
    "    y_data[i, int(labels[subject_IDs[i]])-1] = 1\n",
    "    y[i] = int(labels[subject_IDs[i]])\n",
    "    site[i] = unique.index(sites[subject_IDs[i]])\n",
    "\n",
    "# Compute feature vectors (vectorised connectivity networks)\n",
    "features = get_networks(subject_IDs, kind='correlation', atlas_name='ho')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = y\n",
    "label = label.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for train_index, test_index in skf.split(features, label):\n",
    "    print(train_index)\n",
    "    print(test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "feat_256_1 = feature_selection(features, label, train_index, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_256_1_pd = pd.DataFrame(feat_256_1)\n",
    "feat_256_1_pd['label'] = label\n",
    "feat_256_1_pd.to_csv(\"processed_standard_data_256_1.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_dict = {}\n",
    "for i in range(1):\n",
    "    feat_dict[i] = list(range(i*256,(i+1)*256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('modal_feat_dict.npy',feat_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}